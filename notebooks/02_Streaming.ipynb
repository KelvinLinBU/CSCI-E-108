{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7fb60a",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "## Assignment 02     \n",
    "## CSCI E-108\n",
    "### Steve Elston\n",
    "\n",
    "\n",
    "In this assignment you will work with some basic streaming analytic algorithms. To avoid the complexities of installing and setting up a real streaming analytics platform, you will work with stream flow data loaded from local files. Specifically in this assignment you will:    \n",
    "1. Create and apply code to perform basic stream queries.    \n",
    "2. Using stream queries and plots, explore the stream data.    \n",
    "3. Use moving windows to compute moving averages and sub-sample a stream.    \n",
    "4. Use exponential decay filters to compute moving averages and sub-sample a stream.    \n",
    "5. Work with a Bloom and quotient filters to filter for customer identifiers on a list.\n",
    "6. Use the count-min-sketch algorithm to find counts of unique event identifiers.\n",
    "7. Use the HyperLogLog algorithm to find the cardinality of events in a stream.  \n",
    "\n",
    "## Overview \n",
    "\n",
    "The United States Geological Survey (USGS) maintains over 13,500 stream flow gauges in the United States. Measurements from most of these gauges are recorded every 15 min and uploaded every 4 hours are [available for download](https://waterdata.usgs.gov/nwis/rt). Stream flow data are used as inputs for complex water management tasks for purposes such as agriculture, residential use and conservation. For this assignment you will work with the time series of measurements for two stream flow gauges on tributaries of the the Columbia River in the US State of Washington.     \n",
    "\n",
    "To get started, execute the code in the cell below to import the packages you will need. \n",
    "\n",
    "> **Note:** You must pip install four packages to run the code in this notebook. You can perform the installation of these packages by uncommenting the shell commands shown in the cell below. Alternatively, you can also Conda install these packages.    \n",
    "> 1. [Mmh3](https://github.com/hajimes/mmh3)      \n",
    "> 2. [Bitarray](https://github.com/ilanschnell/bitarray)\n",
    "> 3. [PyProbables](https://pyprobables.readthedocs.io/en/latest/index.html)\n",
    "> 4. [Datasketch](https://ekzhu.com/datasketch/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5041e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mmh3\n",
    "#!pip install bitarray \n",
    "#!pip install pyprobables\n",
    "#!pip install datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ad9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import mmh3\n",
    "from bitarray import bitarray\n",
    "from probables import QuotientFilter, CountMinSketch, HeavyHitters\n",
    "from datasketch import HyperLogLog\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from decimal import Decimal\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff02ac",
   "metadata": {},
   "source": [
    "## Loading the Dataset  \n",
    "\n",
    "The next step is to load the stream gauge data. The code in the cell below loads the time series data for the first gauge. This gauge is sited on the Okanogan river.  \n",
    "\n",
    "The code in the cell below does the following:  \n",
    "1. Loads the data from a .csv file. \n",
    "2. Converts the time stamp column to an index of the Pandas data frame. \n",
    "3. Assigns human-understandable names to the columns.  \n",
    "4. Returns just the first 4 columns of the data frame. \n",
    "\n",
    "Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afbed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_index_series(file_name):  \n",
    "    '''Function to read time series data from a file.\n",
    "    Argument is the path and filename.'''\n",
    "    df = pd.read_csv(file_name, sep='\\t')\n",
    "    df.index = df.datetime\n",
    "    df.drop('datetime', axis=1, inplace=True)\n",
    "    df = df.iloc[:,:4]\n",
    "    df.columns = ['Agency', 'Site_number', 'Time_zone', 'Stream_flow']\n",
    "    return df.iloc[:,:4]\n",
    "\n",
    "Malott = read_index_series('../data/12447200_Okanogan_at_Malott.txt')\n",
    "Malott"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12aa937",
   "metadata": {},
   "source": [
    "The other time series is for a gauge on the Yakima River. Execute the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleElm = read_index_series('../data/12479500_Yakima_At_CleElm.txt')\n",
    "CleElm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7b2f4",
   "metadata": {},
   "source": [
    "Since we really only want to work with one data frame. The code in the cell below merges the two time series and sorts them into time index order. Execute this code and examine the result, paying attention to the site number and the datetime index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ccdeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_flow = pd.concat([Malott,CleElm]).sort_index()\n",
    "stream_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb8b33",
   "metadata": {},
   "source": [
    "## Querying Stream Data\n",
    "\n",
    "Common stream data operations are often formulated as queries on the stream data. Many streaming data platforms use extensions of SQL for these queries.   \n",
    "\n",
    "To keep things simple in this assignment we use a basic query function. The function shown in the cell below supports queries on the time series. This function calls a function that returns time slices of the stream data frame.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dataframe(df, start_time=None, end_time=None):\n",
    "    \"\"\"\n",
    "    Function returns a time slice out of a stream of data\n",
    "\n",
    "    Args:\n",
    "        df: data frame containing the stream data\n",
    "        start_time: the starting time of the slice, can be in datetime form or an integer\n",
    "        end_time: the ending time of the slice, can be in datetime form or an integer\n",
    "    \"\"\"\n",
    "    ## Set the start and end times to the extremes if not specified\n",
    "    if start_time==None: start_time = df.index[0]\n",
    "    if end_time==None: end_time = df.index[df.shape[0]-1]\n",
    "   \n",
    "    ## Test if index is a string datetime or an integer\n",
    "    ## use iloc method if an integer.\n",
    "    ## A slice over the time range is created based on the index type. \n",
    "    if isinstance(df,pd.DataFrame):\n",
    "        if isinstance(start_time, str):\n",
    "            df = df.loc[start_time:end_time,:]\n",
    "        else:     \n",
    "            df = df.iloc[start_time:end_time,:]\n",
    "    else: # Must be a Pandas series \n",
    "        if isinstance(start_time, str):\n",
    "            df = df.loc[start_time:end_time]\n",
    "        else:     \n",
    "            df = df.iloc[start_time:end_time]\n",
    "    return df\n",
    "\n",
    "\n",
    "def query_stream(df, Columns=None, site_numbers=None, start_time=None, end_time=None):    \n",
    "    '''\n",
    "    Function to query the stream gage time series data. The arguments are:    \n",
    "    df = the data frame containing the data.  \n",
    "    Columns = a list of columns to return.   \n",
    "    site_numbers = a list of gage site numbers to query data. \n",
    "    start_time = the start time of the returned data as datatime string or integer index.   \n",
    "    end_time = the end time of the returned data as datatime string or integer index.\n",
    "    '''\n",
    "    ## First set values for arguments set to Null  \n",
    "    if Columns==None: Columns = df.columns\n",
    "    if site_numbers==None: site_numbers = df.Site_number.unique()\n",
    "    \n",
    "    ## Select the sites\n",
    "    df = df.loc[df.Site_number.isin(site_numbers), Columns]\n",
    "\n",
    "    ## A slice over the time range is created based on the arguments. \n",
    "    df = slice_dataframe(df, start_time=start_time, end_time=end_time)\n",
    "    ## Return the results of the query\n",
    "    return df\n",
    "\n",
    "query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31bf56",
   "metadata": {},
   "source": [
    "You can see the options to run `query_stream` function by executing the code in the cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79daad7f",
   "metadata": {},
   "source": [
    "An example of using the query function is shown in the cell below. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af068b6-a869-41b4-a34a-ce7d19dfd547",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0c9f9",
   "metadata": {},
   "source": [
    "> **Exercise 02-01:** Using the `query_stream` function, write and execute the code in the cell below to compute and display the mean `Stream_flow`for the month of April of 2020 of site 12484500. Use the [Pandas.DataFrame.mean](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html) method to compute the mean. Notice that using this approach we can compute most any statistic of interest on the query result.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc98d30",
   "metadata": {},
   "source": [
    "## Plotting Streaming Data\n",
    "\n",
    "Visualization is important tool in data exploration and discovery. Numerical stream data is ideal for visual exploration if it can be subsampled to manageable size.  \n",
    "\n",
    "The function in the cell below creates a time series plot. The time index of the Pandas data frame is used to generate the x-axis values. Execute the code in this cell to load this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768467d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(df, ax=None, ylabel='Stream flow', title=''): \n",
    "    if ax==None: fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    df.plot(ax=ax);\n",
    "    ax.set_xlabel('Date');\n",
    "    ax.set_ylabel(ylabel);\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "    return ax    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea571c",
   "metadata": {},
   "source": [
    "The code in the cell below creates time series plots of the stream flow data. The flow time series for two stream gauges queried as arguments to the plot function. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e82d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plot_time_series(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12447200]), title='Flow on Okanogan at Malott') \n",
    "_=plot_time_series(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500]), title='Flow on Yakima at Cle Elm')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c53ca",
   "metadata": {},
   "source": [
    "The time series of stream flow at both of these gauges is rather complex. Both rivers have several dams used to control the flow. The flow is optimized to conserve fisheries and to supply agriculture in the Columbia River Basin. Water in reservoirs accumulates in the spring as mountain snow melts. The water is then released throughout the spring and summer. \n",
    "\n",
    "But, what can we make of the noticeable spikes in flow, particularly for gauge $12484500$ on the Yakima River. Even with the control provided by dams and reservoirs spring and early summer storm events can cause temporary increases in water flow. These storms bring heavy, and often warm, rain. Flow in the rivers increases not only because of the rainfall, but also since warm rain accelerates snow melt in the higher elevations.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e06c6f",
   "metadata": {},
   "source": [
    "> **Exercise 02-02:** The transitory flow events on the Yakima River warrant some further investigation. You now have the tools to query and plot the stream flow time series. Your goal is to determine if there are common properties (e.g. duration or amplitude) of these events. Plot the results of a query for stream flows on gauge $12484500$, Yakima River, from the 6th day of April to the 20th day of June, 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b87495",
   "metadata": {},
   "source": [
    "> Discuss any common pattern in terms of approximately common high amplitudes or durations of these flow events you can see. Note, this question is a bit open ended.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ccc59",
   "metadata": {},
   "source": [
    "> **Answer:**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8589b",
   "metadata": {},
   "source": [
    "## Applying Moving Window Filters\n",
    "\n",
    "Moving window filters are a commonly used method to compute statistical samples from streaming data. \n",
    "\n",
    "Apply a moving window filter. \n",
    "\n",
    "> **Exercise 02-03:** You will complete and test the function in the cell below. The function queries a time series to create overlapping windows of a specified length and stride. For each window the mean of the stream flow is computed. The function returns a [Pandas Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) object. The time index of the Series object is the mid-point index of the window used to compute the statistic. \n",
    "> 1. Define empty lists for the output values and the time index of the output values.\n",
    "> 2. Use the `window_sample` function, including arguments `length` and `stride`, to create a list of the samples over the series of windows. Notice, that for real streaming data, one would work with the continuous stream to create the window samples. Here we can only simulate this process as a list of (overlapping) window values. Use the default values for `length` and `stride`. \n",
    "> 3. Use a `for` loop to create the overlapping moving window samples of the input. The `window_sample` function provided to create a list of window samples using the `length` and `stride` arguments. At each iteration, the window will advance by `stride` time steps.\n",
    "> 4. In a loop over the window samples do the following:\n",
    ">    - Compute the mean of the window sample and append it to the appropriate list.\n",
    ">    - Find the midpoint time index of the window and append it to the appropriate list.   \n",
    "> 6. Once the loop has terminated, use the [Pandas.Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) constructor to instantiate the return series of the mean of the window samples and return this series.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dcefc7-1050-434d-9980-71726fa1705a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def window_sample(ts, \n",
    "                  length=16, \n",
    "                  stride=8): \n",
    "    windows =[]\n",
    "    for ix_end in range(length, len(ts), stride):\n",
    "        ix_start = ix_end - length \n",
    "        sample = slice_dataframe(ts, start_time=ix_start, end_time=ix_end)\n",
    "        windows.append(sample)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_average(ts, length=16, stride=8, Columns='Stream_flow', site_numbers=[12484500]):\n",
    "    half_length = int(length/2)-1\n",
    "\n",
    "    ## Put your code below\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be829a8",
   "metadata": {},
   "source": [
    "> 3. Next you will test your function by completing and executing the code in the cell below. Use your `window_average` function to create a Pandas Series with 4-hour stream flow averages (length of 16 time steps), taken every 2 hours (stride of 8 time steps). Name your return Series `filtered_16. Compute and print the length of the mean filtered series. The code provided queries the data so that you are working with only values from the Yakima River gauge. Using flow rate values from only one gauge simplifies the bookkeeping for window sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query to create a series with only the Yakima River stream gauge data. \n",
    "Yakima = query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500])\n",
    "\n",
    "filtered_16 = window_average(Yakima)\n",
    "print(filtered_16.head())\n",
    "len(filtered_16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca04cf",
   "metadata": {},
   "source": [
    "> Notice how the length of the time series has been significantly reduced. Is the reduction in length of the time series consistent with a stride of 8 time steps?   \n",
    "> **End of Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3992f0",
   "metadata": {},
   "source": [
    "> **Answer:**        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945f177",
   "metadata": {},
   "source": [
    "To examine the 4-hour moving average time series, the difference series, and the root mean squared error or difference between the two, execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759af3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_=plot_time_series(filtered_16, title='Flow on Yakima at Cle Elm. 4-hour average')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753b0ea-894f-4cb0-9d6a-3688c24cbf67",
   "metadata": {},
   "source": [
    "Notice the following about the results above. The moving average smoother has had only minimal effect on the stream flow time series. The difference series shows only small values with respect to the range of values in the filtered series. Further, the root mean square error (RMSE) is also small compared to the range of values of the filtered series.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd0b31d",
   "metadata": {},
   "source": [
    "> **Exercise 02-4:** You will compute and display a time series using a longer, 1-day (96 time steps) moving window with a stride of 1/2 of a day (48 time steps). You will also perform evaluation of the resulting series. For this exercise, do the following:   \n",
    "> 1. Use the `window_average` function you completed in the previous exercise to compute the smoothed series. \n",
    "> 4. Print the length of the resulting Pandas Series.\n",
    "> 5. Plot the filtered (moving average smoothed) time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ebcd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d9f87",
   "metadata": {},
   "source": [
    "> Answer the following questions:   \n",
    "> 1. What is the data compression ratio with respect to the original stream flow series? Is this consistent with the stride of the window?   \n",
    "> 2. Compare the plots of the results of the two moving window summaries. What are the obvious differences?\n",
    "> 3. If the goal is to measure total volume of water passing the gauge on a daily and weekly basis, does the series from the longer filter contain sufficient detail? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f22026",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.        \n",
    "> 2.           \n",
    "> 3.                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c4fc4-c96a-4af3-a43d-2df08efe35fe",
   "metadata": {},
   "source": [
    "## Reservoir Sampling   \n",
    "\n",
    "At large scale, one needs to randomly sample from streams. Some of the many examples where working with a random sample of the streams is required include the following.     \n",
    "1. An Internet security system analyzes the IP address of of Internet traffic to detect denial of service attacks. The system needs to compute various statistics on the IP address pair (origin and destination) streams. But, given the massive volume of Internet traffic, it is impossible to store much history of these streams.    \n",
    "2. A large e-commerce company must analyze the click streams on it many web sites. These analytics must consider every click on the sites, not just clicks leading to purchases. It is impractical economically to store long histories of these click streams.\n",
    "\n",
    "Could one use Poisson sampling to reduce the volume of data used for analysis? Let's do some analysis. For basic Poisson sampling an observation is included in the sample with probability $p$. Once we have seen $i$ samples in a stream our sample will be of size $p\\ i$. As time goes on the memory requirement, $m(t)$, continues to increase.   \n",
    "$$m(t) = p\\ i(t) \\rightarrow \\infty\\ as\\ t \\rightarrow \\infty$$\n",
    "It is clear that Poisson sampling is not an option for infinite streams of data! We need another approach.   \n",
    "\n",
    "Reservoir sampling maintains a buffer of constant size $k$. The first $k$ sample to arrived are placed in the buffer. When the $i$th sample arrives a random integer $r(i) \\sim Unif(0,i)$ is generated. If $r(i) \\le k$, the $r$th sample of the buffer is replaced by the new sample. The probability of the $i$th sample being included in the buffer is:   \n",
    "$$p(s(i)) = \\frac{k}{i+1}$$     \n",
    "\n",
    "The code in the cell below computes and displays the size of the required memory and the probability of a sample being included are computed and displayed as a function of $i$. Execute this code and examine the result.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045253b-06ce-43e0-8ef9-544334e1ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = 1000\n",
    "weight = 1.0\n",
    "k=100\n",
    "p=0.01\n",
    "\n",
    "porbability =  lambda i: k * weight / (i + 1) if i > k  else 1\n",
    "length = lambda i: k if i > k else i\n",
    "\n",
    "i_vector = range(N_samples)\n",
    "probabilities = [porbability(i) for i in i_vector]\n",
    "lengths_resivoir = [length(i) for i in i_vector]\n",
    "lengths_poisson = [i for i in i_vector]\n",
    "\n",
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].plot(i_vector, lengths_resivoir, label='Reservoir Sampling');\n",
    "ax[0].plot(i_vector, lengths_poisson, label='Poisson Sampling');\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Number of observations seen');\n",
    "ax[0].set_ylabel('Length of sample vector');\n",
    "ax[0].set_title('Length of sample vs. samples observed');\n",
    "\n",
    "\n",
    "ax[1].plot(i_vector, probabilities, label='Reservoir Sampling');\n",
    "ax[1].plot(i_vector, [p]*len(i_vector), label='Poission Sampling');\n",
    "ax[1].legend()\n",
    "ax[1].set_yscale('log');\n",
    "ax[1].set_xlabel('Number of observations seen');\n",
    "ax[1].set_ylabel('Log probability of inclusion in buffer');\n",
    "ax[1].set_title('Log probabiliy of inclusion vs. samples observed');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eebd4e-02b5-4573-8dad-94cc67f39cd2",
   "metadata": {},
   "source": [
    "Examine these graphs and notice the following, noticing that the probability graph uses a log vertical scale:     \n",
    "- The memory required for Poisson sampling grows linearly with the number of arriving samples.\n",
    "- The memory required for reservoir sampling increases linearly until the buffer is filled and then is constant.\n",
    "- The probability of a sample being included is constant for Poisson sampling.\n",
    "- The probability of a sample being included is constant until the buffer fills and then decreases linearly (exponential on log scale).\n",
    "\n",
    "The foregoing observations show that both sampling methods result in random samples. Reservoir sampling maintains a random sample with fixed memory per stream.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4f2fc-b3d1-4500-afc9-1cf94e1f31fd",
   "metadata": {},
   "source": [
    "### An Efficient Algorithm\n",
    "\n",
    "The basic reservoir sampling algorithm is simple, but not very efficient. A random number needs to be generated for each arriving sample. Random number generation is an expensive process. This simple algorithm scales poorly large numbers of streams and high arrival rates.    \n",
    "\n",
    "A better approach can be obtained by considering the [**geometric probability distribution**](https://en.wikipedia.org/wiki/Geometric_distribution) of the interval between samples. First we generate a random number with the required distribution: \n",
    "\n",
    "$$W = exp \\Bigg( \\frac{log \\big(Unif(0,1) \\big)}{k}   \\Bigg)$$\n",
    "Now the index of the next sample to be placed in the reservoir is sampled:   \n",
    "$$i = i + floor \\Bigg( log \\Big( \\frac{Unif(0,1) }{1-W)} \\Big) \\Bigg) + 1$$\n",
    "\n",
    "Note that when using Python with zero-based indexing, the $+1$ term should be dropped.  \n",
    "\n",
    "The last step is to find the index of the sample replaced in the reservoir from a uniform distribution, $r = Unif(1,k)$. Here $Unif(x,y)$ is the uniform distribution on the interval $[x,y]$ and $floor$ is a function that rounds down to the next lowest integer.       \n",
    "\n",
    "Comparing the foregoing algorithm to the initial simple approach we see that the efficient algorithm needs to compute three random numbers each time a sample is added to the reservoir. However, the three random number need only be computed for samples that are actually placed in the reservoir. Given that the probability of a sample being placed in the reservoir decreases linearly with the number of samples seen in the stream, the number of random numbers generated continues to decrease as the sampling progresses.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafac2ef-af6b-4eab-8dab-eba96cf6d4b4",
   "metadata": {},
   "source": [
    "> **Exercise 2-5:** The code in the cell below implements an efficient version of the reservoir sampling algorithm by the following steps.\n",
    "> 1. The reservoir is filled with the first $k$ samples from the stream.\n",
    "> 2. For the rest of the stream an index for the next sample is computed by a random draw from a Geometric distribution.\n",
    "> 3. Each time a sample is to be added to the reservoir a random draw of an index which determines which sample in the reservoir is replaced.\n",
    ">\n",
    "> Execute this function with the `print_test` and `print_updates` arguments set to `True`.          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac908b9-8072-4aa5-ba0b-81b01e85c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_sampling(stream, k, analytic_function=None, print_test=False, print_updates=False): \n",
    "    \"\"\"\n",
    "    Performs reservoir sampling on a stream of data.\n",
    "\n",
    "    Args:\n",
    "        stream: iterable data stream\n",
    "        k: size of the reservoir\n",
    "        analytic_function: A function that operates on the samples in the reservoir each time a sample is added\n",
    "        print_test: produces voluminous test output when set to True. No analytic results are returned    \n",
    "        print_updates: print number of buffer updates performed on a stream if set to True\n",
    "    \"\"\"\n",
    "    reservoir_buffer = []\n",
    "    out_list = []\n",
    "    out_index = []\n",
    "    n = len(stream)\n",
    "    n_updates = 0\n",
    "\n",
    "    ## First, fill the buffer with the first k observations \n",
    "    for i in range(k):\n",
    "        reservoir_buffer.append(stream.iloc[i])\n",
    "        if print_test: print(reservoir_buffer)\n",
    "        if analytic_function != None: \n",
    "            n_updates += 1\n",
    "            out_list.append(analytic_function(reservoir_buffer))\n",
    "            out_index.append(i)\n",
    "\n",
    "    ## Draw a random number from the distribution  \n",
    "    W = math.exp(math.log(random.random())/k)    \n",
    "    ## Loop over the remaining samples in the stream\n",
    "    while i < n:\n",
    "        i = i + math.floor(math.log(random.random())/math.log(1 - W)) # + 1\n",
    "        if i < n:\n",
    "            indx = random.randint(1, k) -1 \n",
    "            reservoir_buffer[indx] = stream.iloc[i]\n",
    "            W = W * math.exp(math.log(random.random())/k)  \n",
    "            n_updates += 1\n",
    "            if print_test: \n",
    "                print('For sample ' + str(i) + '  with buffer element replaced = ' + str(indx + 1))\n",
    "                print(reservoir_buffer)\n",
    "            if analytic_function != None: \n",
    "                out_list.append(analytic_function(reservoir_buffer))\n",
    "                out_index.append(i)\n",
    "    if print_updates: \n",
    "        print('Number of samples = '+ str(len(stream)))\n",
    "        print('Number of buffer updates = ' + str(n_updates))\n",
    "    if analytic_function != None: return pd.Series(out_list, index=out_index)\n",
    "\n",
    "k = 8\n",
    "Yakima = query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500], start_time='2020-04-06 00:00', end_time='2020-04-07 11:59')\n",
    "random.seed(345)\n",
    "reservoir_sampling(Yakima, k, print_test=True, print_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0115e0-479e-4028-8930-1fce8cb7155b",
   "metadata": {},
   "source": [
    "> Examine these results and answer the following questions:      \n",
    "> 1. Do the indices of the samples selected appear to be a sequence with an increasing interval? Keep in mind that for small $n$ (number of samples seen) the Geometric distribution has a high probability of sampling a 0 interval.    \n",
    "> 2. Do the samples replaced in the reservoir buffer appeared to have uniformly random indices.    \n",
    "> 3. Compare the number of samples added to the reservoir buffer to the number of samples in the stream. What does this comparison tell you about the efficiency of this algorithm, which requires three random number draws, to the simple algorithm that requires a random number draw for each arriving sample?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7994c-e8e3-45f9-857b-430e29d03942",
   "metadata": {},
   "source": [
    "> **Answers:**       \n",
    "> 1.     \n",
    "> 2.     \n",
    "> 3.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb99461-a961-478b-b06f-aab5dd06b896",
   "metadata": {},
   "source": [
    "> The code in the cell below does the following:\n",
    "> 1. Performs a query for the entire Yakima river flow time series.\n",
    "> 2. Performs reservoir sampling on the stream and computes and returns the mean of the sample each time a sample is added to the reservoir buffer.\n",
    ">\n",
    "> Execute this code, noting the number of samples taken and the execution time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411bc017-d400-409f-95c9-1867e5f44580",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=64\n",
    "Yakima = query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500])\n",
    "random.seed(121)\n",
    "%time analytic_series = reservoir_sampling(Yakima, k, lambda x: np.mean(x), print_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f13bd8-1936-4594-97e8-05405f17916e",
   "metadata": {},
   "source": [
    "> The code in the cell below computes the mean statistic on the entire expanding window of the stream samples. Execute this code noting the execution time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f813f-d80c-4c7b-a5f1-8cb0869d4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_full_analytic(stream, index, analytic_function):\n",
    "    out_list = []\n",
    "    for i in index:\n",
    "        out_list.append(analytic_function(stream[:i]))\n",
    "    return pd.Series(out_list, index = index)\n",
    "\n",
    "%time full_analytic = compute_full_analytic(Yakima, analytic_series.index, lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881bac6-38f4-46b1-b247-0cdfc7b665b2",
   "metadata": {},
   "source": [
    "> Finally, to compare the mean statistic computed from the reservoir sample to the statistic computed with the cumulative sample, execute the code in the cell below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ecb1c-4555-419c-87be-3ee4f614a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax=plt.subplots(figsize=(8,6))\n",
    "ax.scatter(analytic_series.index, analytic_series, label='Resivoir sample');\n",
    "ax.plot(analytic_series.index, full_analytic, lw=2, c='orange', label='Full sample');\n",
    "ax.set_title('Mean estimate vs. stream sample index');\n",
    "ax.set_xlabel('Stream sample index');\n",
    "ax.set_ylabel('Mean estimate from reservoir buffer');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ac38b-cf45-41aa-acc9-edc2251d3618",
   "metadata": {},
   "source": [
    "> Examine the foregoing results and answer these questions:       \n",
    "> 4. How can you account for the for the difference in time required to compute the mean using reservoir sampling versus using the full samples in the expanding window?      \n",
    "> 5. Compare the full sample mean estimates and the mean estimates from the reservoir samples shown in the plots. How would you characterize the reservoir sampling approximation, keeping in mind that we are using a very small buffer with $k=64$ and the values in the stream are not stationary (have a trend)?     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79843e28-beae-42d8-9f37-b020c18be647",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 4.       \n",
    "> 5.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438bb96b",
   "metadata": {},
   "source": [
    "## Exponential Decay Filters\n",
    "\n",
    "The idea of using exponential smooth for time series analysis is an old one, dating at least to use by Weiner in the 1920s. The related idea of moving average filters was developed by Kolmogorov and Zurbenko in the 1940s. Exponential smoothers were used extensively in signal process in the 1940s. The general idea was expanded by Robert Goodell Brown (1956) and C.E. Holt (1957) and his student P.R. Winters (1960). The higher-order Holt-Winters model accounts for trend and seasonality of time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c336514",
   "metadata": {},
   "source": [
    "### Basic exponential Smoothing\n",
    "\n",
    "Exponential smoothing uses a weighted sum of the current observation and the past smoothed value to compute a new smoothed value. This basic exponential smoothing relationship is shown below.  \n",
    "\n",
    "$$\n",
    "s_0 = x_0\\\\    \n",
    "s_t = \\alpha x_t + (1-\\alpha) s_{t-1} = s_{t-1} + \\alpha(x_t - s_{t-1}),\\ t \\gt 0\n",
    "$$\n",
    "\n",
    "The smoothing hyperparameter, $\\alpha$, controls the trade-off between the last observation and the previous smoothed values. The possible values are in the range, $0 \\le \\alpha \\le 1$. A large value of $\\alpha$ puts more weight on the current observation. Whereas, a small value of $\\alpha$ puts more weight on the smoothed history.      \n",
    "\n",
    "How can we understand the exponential decay of the smoothed history of a time series? The smoothing hyperparameter, $\\alpha$, an be expressed in terms of the decay constant, $\\tau$ and time interval $\\Delta T$ as shown below.  \n",
    "\n",
    "$$\n",
    "\\alpha = 1 - e^{\\big( \\frac{- \\Delta T}{\\tau} \\big)}\n",
    "$$\n",
    "\n",
    "From this relationship you can see that the influence of the smoothed history decays exponentially as $\\delta T$ increases. The decay time increases as $\\tau$ decreases.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e56565",
   "metadata": {},
   "source": [
    "### Smoothing with higher-order terms   \n",
    "\n",
    "The basic exponential smoothing algorithm is effective in many cases. However, the simple first order exponential smoothing method cannot accommodate time series with trend or seasonality. Higher order smoothing models are required.   \n",
    "\n",
    "The **double exponential smoothing** or **Holt-Winters double exponential smoothing** algorithm is a second order smoothing method. Using two coupled difference equations a trend and non-seasonal component of the time series can be modeled. The model updates a smoothed measure of the non-seasonal component and the trend.   \n",
    "\n",
    "The model is initialized with the values:   \n",
    "$$\n",
    "s_1 = x_1 \\\\\n",
    "b_1 = x_2 - x_1\n",
    "$$\n",
    "\n",
    "At each time step the a pair of time difference equations are updated. The following relationships update the smoothed non-seasonal component, $s_t$, and the slope, $b_t$:   \n",
    "\n",
    "$$\n",
    "s_t = \\alpha x_t + (1-\\alpha) (s_{t-1} + b_{t-1}) \\\\\n",
    "b_t = \\beta(s_t - s_{t-1}) + (1 - \\beta)b_{t-1}\n",
    "$$\n",
    "\n",
    "The smoothed non-seasonal component and smoothed slope can be used to compute a forecast. The relationship below computes the forecast $m$ time steps ahead.      \n",
    "\n",
    "$$ F_{t+m} = s_t + m b_t $$   \n",
    "\n",
    "What about seasonal components? A third-order difference relationship can updated a smoothed seasonal component, along with the smoothed non-seasonal and slope components. The details of this process are not discussed further here. The details are available elsewhere, including the [exponential smoothing Wikipedia page](https://en.wikipedia.org/wiki/Exponential_smoothing#:~:text=Exponential%20smoothing%20is%20a%20rule,exponentially%20decreasing%20weights%20over%20time.).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4642b7",
   "metadata": {},
   "source": [
    "### Example of Exponential Decay Filtering     \n",
    "\n",
    "> **Exercise 02-6:** You will now create and test a single or simple exponentially weighted decay filter. This function will have a stride argument just as the window filter function. Your function, `exponential_smooth`, will have arguments of the time series, the exponential smoothing parameter and a stride. In this case the stride $=48$ and $\\tau=24.0$. Your function will do the following:    \n",
    "> 1. Initialize a list for the filtered samples with the first value of the input series. The samples list will contain the exponentially weighted smoothed samples. Make sure you save the first value in the list.   \n",
    ">    a. Initialize empty lists for the output values and time index of the output values.   \n",
    ">    b, Iterates over all the values of the time series starting with the second one. The first observation is the intial value. In the loop update the exponentially weighted smoothed values using the first order smoothing algorithm.\n",
    ">    c. If the loop index modulo the stride $= 0$ then, do the following:     \n",
    ">           - Append the smoothed sample value to the output list.       \n",
    ">           - Append the time index of that sample to the index list.    \n",
    ">    d. Create an output Pandas Series from the output list using the output index list as the index of the series.   \n",
    ">    e. Return the Pandas Series.   \n",
    "> 8. Compute and print the value of $\\alpha$ given $\\tau = 24.0$.   \n",
    "> 9. Execute your function for site number $12484500$ and arguments of the computed $\\alpha$ and stride $=48$, or 12 hours.  \n",
    "> 10. Print the length of the resulting series.     \n",
    "> 11. Print the value of $\\alpha$, given the time step and value of $\\tau$.    \n",
    "> 13. Plot the smoothed series using the `plot_time_series` function.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 15/60  # Step time in hours \n",
    "stride = 48 # Stride in samples\n",
    "tau = 24.0 # Decay time in hours \n",
    "def compute_tau(alpha, time_step): \n",
    "    '''Compute the value of tau given the time step and alpha'''\n",
    "    return -time_step / math.log(1 - alpha)\n",
    "def compute_alpha(tau, time_step):  \n",
    "    '''Compute the value of alpha given the time step and tau'''\n",
    "    return 1 - math.exp(-time_step/tau)\n",
    "\n",
    "def exponential_smooth(ts, alpha=0.01, stride=8):\n",
    "    ## Put your code below  \n",
    "    st = ts.iloc[0]\n",
    "    idx = []\n",
    "    out = []\n",
    "    difference = []\n",
    "    ## Put your code below\n",
    "    \n",
    "    \n",
    "    out = pd.Series(out, index=idx)     \n",
    "    return out      \n",
    "\n",
    "alpha = compute_alpha(tau, time_step)\n",
    "smoothed_24 = exponential_smooth(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500]), alpha=alpha, stride=stride)\n",
    "\n",
    "print('The resulting series length = ' + str(len(smoothed_24)))\n",
    "print('alpha = ' + str(round(alpha, 4)))\n",
    "_=plot_time_series(smoothed_24, title='Flow on Yakima at Cle Elm, EWMA filter, alpha=' + str(round(alpha, 3)) + ', tau=' + str(tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f002f",
   "metadata": {},
   "source": [
    "> Provide short answers to the following questions:   \n",
    "> 1. Are the number of smoothed samples correct for the stride of the exponential decay filter selected?     \n",
    "> 2. How does the exponentially smoothed series compare to the original series in terms of frequency components?      \n",
    "> 3. Given the value of $\\tau$ (in hours) what does this tell you about the decay length and smoothing of the filter?     \n",
    "> **End of exercise.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf13035",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.        \n",
    "> 2.    \n",
    "> 3.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1a1de",
   "metadata": {},
   "source": [
    "> **Exercise 02-7:** A question we should ask is what happens if you increase the smoothing constant of the exponential decay filter? In other words, what is the effect of giving greater weight to the most recent values? To find out do the following:  \n",
    "> 1. Compute a value of alpha given $\\tau = 1.0$, one hour.     \n",
    "> 2. Execute the `exponential_smooth` function with arguments of the computed value of $\\alpha$ and `stride=4`.\n",
    "> 3. Print the length of the resulting Pandas Series.\n",
    "> 4. Print the value of $\\alpha$ computed.\n",
    "> 5. Plot the smoothed time series using the `plot_time_series` function.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212c4ba",
   "metadata": {},
   "source": [
    "> Compare the plot of this time series to the previous series less smoothing and provide short answers to the following questions.   \n",
    "> 1. How do the lengths of the smoothed series compare and is this difference consistent with the stride lengths?    \n",
    "> 2. What is the main difference you can see between these smoothed series in terms of frequency components and why is this expected given the decay times?     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb66cd2",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.          \n",
    "> 2.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1cda9",
   "metadata": {},
   "source": [
    "> **Exercise 02-8:** So far, we have picked a stride that is about 1/2 the decay time, which is a typical choice. The next question to ask is what is the effect of using a longer stride? A longer stride reduces the number of smoothed samples used for further processing. To find out, repeat the calculations and plotting of exercise 2-06, but with stride $=96$ and $\\tau = 24.0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 24.0   \n",
    "stride = 96\n",
    "\n",
    "## Put your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7ad21",
   "metadata": {},
   "source": [
    "> Compare the result you just created with that from Exercise 2-5 and answer the following questions:\n",
    "> 1. Is the filter series significantly different, or nearly the same as the series with shorter stride?\n",
    "> 2. Why do you think foregoing outcome is expected?        \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4bc6b6",
   "metadata": {},
   "source": [
    "> **Answers:**           \n",
    "> 1.   \n",
    "> 2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d4e8e",
   "metadata": {},
   "source": [
    "## Filtering Discrete Events  \n",
    "\n",
    "In many cases our goal is to filter discrete events based on some type of identifier. The identifier is a hash of nearly any hashable data type. Many examples of these data types include, string customers identifiers, numeric event identifiers, IP addresses, email addresses.  \n",
    "\n",
    "One example of such a method is the [**Bloom filter**](https://en.wikipedia.org/wiki/Bloom_filter). A Bloom filter is quite efficient in terms of computing and memory.  However, the Bloom filter does not allow for deleting matches once they are added to the hash table. An alternative is a **[quotient filter](https://en.wikipedia.org/wiki/Quotient_filter)**. The quotient filter keeps a count of events for each hash. As a result, an event identifier can be removed from the table by decrementing the counts for the hashes. To perform this extra operation the quotient filter uses more memory and is a bit less computationally efficient.   \n",
    "\n",
    "Both Bloom filters and quotient filters operate by the same principle. A hash table of key-values pairs is created. The keys are the hashes of the event identifiers. For a Bloom filter, the value is binary, the event is in the table or it is not. The quotient filter on the other hand, maintains a count of events in each bucket. The count can be incremented as new instances are encountered, or decremented when deleting events.     \n",
    "\n",
    "In the following exercises, you will construct a Bloom filter using a bit array and the Python [mmh3 package](https://github.com/hajimes/mmh3?tab=readme-ov-file). The mmh3 package implements the widely used [MurmurHash (MurmurHash3)](https://en.wikipedia.org/wiki/MurmurHash) non-cryptographic hash algorithm.                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d85b02",
   "metadata": {},
   "source": [
    "### Instantiate the identifier lists      \n",
    "\n",
    "To start the example, we generate lists of random customer identifiers (IDs) for customers and non-customers by the following steps.     \n",
    "1. Initialize the arguments for the generate of the customer ID lists.  \n",
    "2. Randomly generated lists of customer and non-customer ids are created. A multiplier is used to ensure that some IDs values are outside the range of the length of the hash table. The customer and non-customer identifiers are all integers.   \n",
    "3. The non-customer list is filtered to ensure there are no identifiers common with the customer list.     \n",
    "4. The lengths of the lists and a sample of the customer key values are printed.  \n",
    "\n",
    "Execute the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3905dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_hashes = 1024\n",
    "ID_multiplier = 39\n",
    "number_customers = 100 \n",
    "number_not_customers = 200\n",
    "\n",
    "nr.seed(4565)\n",
    "customers = [int(ID_multiplier * number_of_hashes * i) for i in nr.uniform(size=number_customers)]\n",
    "not_customers = [int(ID_multiplier * number_of_hashes * i) for i in nr.uniform(size=number_not_customers)]\n",
    "\n",
    "## Ensure there are no common ids between customers and non-customers\n",
    "del_list = []\n",
    "for i in range(len(not_customers)): \n",
    "    if not_customers[i] in customers: del_list.append(i)\n",
    "for i in sorted(del_list, reverse=True):\n",
    "    del not_customers[i]\n",
    "\n",
    "print('Number of non-customers = ' + str(len(np.unique(not_customers))))\n",
    "print('Number of customers = ' + str(len(np.unique(customers))))\n",
    "print(customers[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5404b8d",
   "metadata": {},
   "source": [
    "### Using the Murmur Hash\n",
    "\n",
    "You will use the 32 bit Murmur hash function implemented in the Python mmh3 package to create a Bloom filter. First we will investigate some basic properties of the Murmur hash. Specifically, we will investigate how uniform the hash values computed are by the following steps:   \n",
    "1. Created a hash table of length 1024. The table is filled with hashes computed from random integers. Notice that the `mmh3.hash` function only accepts a string argument, requiring the integer to be wrapped in the `str()` function.\n",
    "2. A histogram and cumulative density plot of the hash values are displayed.\n",
    "3. The number of hash collisions is computed and displayed.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_length = 1024\n",
    "number_samples = 128 \n",
    "multiplier = 1019\n",
    "\n",
    "## Compute random numbers and find the corresponding hash values   \n",
    "samples = [int(multiplier * filter_length * i) for i in nr.uniform(size=number_samples)]\n",
    "hash_list = [mmh3.hash(str(i)) % filter_length for i in samples]\n",
    "\n",
    "## Histogram of the hash values\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "counts, bins, _ = ax.hist(hash_list, bins=filter_length);\n",
    "ax.set_ylabel('Count of keys');\n",
    "ax.set_xlabel('Hash value');\n",
    "ax.set_title('Count of keys vs. hash values');\n",
    "ax.set_xlim(0.0, filter_length)\n",
    "plt.show();\n",
    "\n",
    "## Cumulative density plot of the hash values\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(range(len(counts)), np.cumsum(counts))\n",
    "ax.plot([0.0,len(counts)], [0.0,len(hash_list)], color='red');\n",
    "ax.set_ylabel('Cumulative Sum');\n",
    "ax.set_xlabel('Hash index');\n",
    "ax.set_title('Cumulative sum of hash values');\n",
    "ax.set_xlim(0.0, filter_length)\n",
    "plt.show();  \n",
    "\n",
    "## Compute and display the number of hash collisions\n",
    "print('Number of collisions = ' + str(int(sum([count - 1 if count>1 else 0 for count in counts]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c75ba-e2e8-4167-860f-4d3bbf42f6cc",
   "metadata": {},
   "source": [
    "> **Exercise 02-9:** Compare the plots shown above to the plots of the hash function using a prime number key created for Exercise 01-1. Is the Murmur hash function more or less optimal compared to the function tested in Exercise 1-1 and why?    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db54561-1e2e-4582-baca-87fc9d05f9e0",
   "metadata": {},
   "source": [
    "> **Answer:**          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8959eb-650e-431d-869c-1664312157ea",
   "metadata": {},
   "source": [
    "### Create Bloom filter and insert function    \n",
    "\n",
    "You will now instatiate a Bloom filter and create an insert function. As a first step, execute the code in the cell below to instantiate the Bloom filter as an empty bit array.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc96d4b-c2ae-40ff-ae93-e349d7a80f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_length = 1024\n",
    "bloom_filter = bitarray([0]*filter_length)\n",
    "bloom_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77282e20-8b79-47c5-b5b9-b3eaf323943e",
   "metadata": {},
   "source": [
    "> **Exercise 2-10:** You will now complete and test a function to insert an event identifier into the Bloom filter array. To complete the code in the cell below do the following: \n",
    "> 1. Iterate over the hash keys. The key for mmh3 is just an integer.   \n",
    "> 2. For the value, `x`, and integer key compute the hash. Make sure you take the modulo with the length of the Bloom filter.\n",
    "> 3. Set the bit indexed by the hash value to 1.\n",
    "> 4. Return the Bloom filter array.        \n",
    "> Execute your code to add the customers to the hash table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hashes = 3\n",
    "def bloom_insert(x, bloom_filter, n_hashes=3):\n",
    "    ## Put your code below\n",
    "\n",
    "    \n",
    "    return bloom_filter    \n",
    "\n",
    "for customer in customers:  \n",
    "    bloom_filter = bloom_insert(str(customer), bloom_filter, n_hashes=n_hashes)\n",
    "bloom_filter    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1157d87",
   "metadata": {},
   "source": [
    "> Notice that most bits in the Bloom filter array are still 0, and only a small fraction of the bits set to 1.\n",
    ">\n",
    "> Next, you will complete the `bloom_query` function in the cell below. Recall that a query returns True only if all bits indexed by the hash values are 1. To complete the function, do the following:     \n",
    "> 1. Set a counter variable to 0.     \n",
    "> 2. Iterate over the number of hashes.     \n",
    "> 3. Compute the hash for the input value, `x`, and integer hash key.    \n",
    "> 4. If the bit value indexed for by the key equals 1, add 1 to the counter value.      \n",
    "> 5. If the counter value equals the number of hashes, return True, and if not, return False.     \n",
    "> Now, execute the code in the cell below to test your function and find the number of false positives.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bloom_query(x, bloom_filter, n_hashes=3):   \n",
    "    ## Put your code below  \n",
    "    \n",
    "    \n",
    "\n",
    "false_positives = 0 \n",
    "for customer in not_customers:  \n",
    "    if bloom_query(str(customer), bloom_filter, n_hashes=n_hashes): false_positives += 1\n",
    "print('Total number of false positives = ' + str(false_positives))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663164c",
   "metadata": {},
   "source": [
    "> For a perfect (uniformly distributed) hash function the theoretical false positive rate can be computed by the following relationship:  > $$P(false\\ positive) = \\Big[1 - exp \\big(\\frac{- k n}{m} \\big) \\Big]^k$$    \n",
    "> Where, \n",
    "> - $k = $ number of hash functions.    \n",
    "> - $m = $ length of the hash table.   \n",
    "> - $n = $ number of identifiers in the hash table.\n",
    ">  \n",
    "> Now answer the following questions:     \n",
    "> 1. Given this result, what is the empirical false positive rate?     \n",
    "> 2. How does the observed false positive rate compare to the theoretical rate? You should create a function to compute the theoretical false positive rate, which you can use for the next exercise. Name your function `false_positive_rate` with arguments, number of events, length of the bit array, and number of hashes.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d14642",
   "metadata": {},
   "source": [
    "> **Answers:** \n",
    "> 1.     \n",
    "> 2.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a417e7-2a5b-4900-964e-15043e972860",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the theoretical false positive rate here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a136d0-db91-4a0f-811c-5fb464f2c747",
   "metadata": {},
   "source": [
    "> Answer 2 Continued;     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426142f8-dc7e-42a0-8430-f4d0be74ed78",
   "metadata": {},
   "source": [
    "> Next, execute the code in the cell below that uses your `false_positive_rate` function to plot log false positive rates for a number of filter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_list = [2**12, 2**16, 2**24, 2**32]\n",
    "k_list = [2**2, 2**4, 2**6, 2**8, 2**10]\n",
    "n = 10e+7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "for k in k_list:    \n",
    "    fp_list = [false_positive_rate(n, m, k) for m in m_list]\n",
    "    ax.plot(m_list, fp_list, label=str(k) + ' hashes');\n",
    "#    ax.plot(k_list, fp_list, label=\"{:.1E}\".format(Decimal(m)) + ' bit array');\n",
    "ax.set_yscale('log');\n",
    "ax.set_ylabel('Log false positive rate');\n",
    "ax.set_xlabel('Length of bit array');\n",
    "ax.set_title('False postive rate vs. length of bit array for 10 million events');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343ce1d",
   "metadata": {},
   "source": [
    "> Finally, answer these questions:    \n",
    "> 3. What is the relationship between length of the bit array and false error rate, and why do you think this makes sense?     \n",
    "> 4. How does increasing the number of hashes affect the false positive rates, and why do you think this makes sense?       \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4116a",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 3.     \n",
    "> 4.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa95248-baba-4fe9-a748-4cc610238082",
   "metadata": {},
   "source": [
    "## Applying the Quotient Filter\n",
    "\n",
    "The quotient filter is an improvement on a Bloom filter, and uses a more compact hashing method. Like Bloom filters, quotient filters are not particularly difficult to implement. There are also several Python packages that support Bloom and quotient filters. For the following exercise you will use the [PyProbabilies](https://pyprobables.readthedocs.io/en/latest/index.html) package.     \n",
    "\n",
    "> **Exercise 02-11:** In this exercise you will work with a quotient filter. To instantiate and add hashes for customer IDs do the following:   \n",
    "> 1. Instantiate the quotient filter using the [QuotientFilter](https://pyprobables.readthedocs.io/en/latest/code.html#probables.QuotientFilter) function using default argument values. Name your filter object `quotient_filter`.      \n",
    "> 2. To get a feel for the quotient filter created print the following attributes of the filter object, `quotient`, `remainder` and `num_elements`.     \n",
    "> 3. Iterate over the customer list and add the customer identifier to the quotient filter using the `add` method. Note that the key argument must be of a string type.\n",
    "> 4. Print the `elements_added` attribute of the quotient filter.\n",
    "> Execute your code.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6d6a6-0e50-45ae-b7c5-9a77be272221",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c1c2f-d596-4f13-aaa9-7f39bfa9c434",
   "metadata": {},
   "source": [
    "> Next, you will query the quotient filter with the identifiers in the not_customer list, by executing the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb917807-4331-4811-9eda-232d22321fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = 0 \n",
    "for customer in not_customers:  \n",
    "    if quotient_filter.check(str(customer)): false_positives += 1\n",
    "print('Total number of false positives = ' + str(false_positives))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c04e25-1bd4-4c3a-aaca-7e69ce29ebbe",
   "metadata": {},
   "source": [
    "> Compare the false positive rates of the Bloom filter example of Exercise 2-9 and the quotient filter. Can you see that one filter has an advantage for this case and why?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f83730-d9df-4234-9fe9-2faa887ee80d",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3491f577-a9b2-4456-a068-90808a66a4a8",
   "metadata": {},
   "source": [
    "## Count Min Sketch\n",
    "\n",
    "The probabilistic count min sketch algorithm uses multiple hash functions in an efficient data structure to estimate \n",
    "\n",
    "[Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) is generally considered a model for activity of online user activity, spot event attendance, and other human activities. One possible model for this behavior is the [Zipf-Mandelbrot distribution](https://en.wikipedia.org/wiki/Zipf%E2%80%93Mandelbrot_law), with the following **probability mass function**:   \n",
    "\n",
    "$$f(k,N,\\alpha,\\beta)=\\frac{1/(k + \\beta)^\\alpha}{H_{N,q,s}}$$\n",
    "where:   \n",
    "$k=$ rank in set of size $N$.       \n",
    "$\\alpha$ abd $\\beta$ are parameters.     \n",
    "$H_{N,q,s}=\\sum^N_{k=1} 1/(k + \\beta)^\\alpha$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d1956-91e5-4c08-85c0-9595e427e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "beta = 2.7\n",
    "zipf = [1/(i + beta)**alpha for i in range(len(customers))]\n",
    "zipf = np.divide(zipf, np.sum(zipf))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.plot(range(len(customers)), zipf)\n",
    "ax.set_ylabel('Probability density')\n",
    "ax.set_xlabel('Rank')\n",
    "ax.set_title('Density of Zipf distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b6f23-f77b-4342-b0b3-723ce8ff0c60",
   "metadata": {},
   "source": [
    "Notice the rapid decay of this distribution with event identifier. This rapid decay of event frequency is often observed in many real-world phenomenon.    \n",
    "\n",
    "Next, execute the code in the cell below to generate a data set using the Zipf-Mandelbrot distribution of events just computed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1740e8-e8e4-4e79-8022-a6b2a49d91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = int(10e+5)\n",
    "stream = np.random.choice(customers, size=n_samples, p=zipf)\n",
    "stream[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a2f04-c1f6-430e-8856-f7c7c0fc44a2",
   "metadata": {},
   "source": [
    "Notice the random nature of the identifiers in the first 200 events in the stream.  \n",
    "\n",
    "### Applying the Heavy Hitters Algorithm\n",
    "\n",
    "A variation on the count-min-sketch algorithm is used to find the **heavy hitters**, or the event ids that occur most frequently. This approach allows one to find the counts of a pre-determined number of common events, using the small memory footprint of the count-min-sketch algorithm, but at a cost of some additional computation for sorting and bookkeeping.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0824347-199a-475b-b224-b18230618643",
   "metadata": {},
   "source": [
    "> **Exercise 02-12:** You will now use the [probables.HeavyHitters](https://pyprobables.readthedocs.io/en/latest/code.html) function to find the 20 event identifiers with the highest event count. Do the following:\n",
    "> 1. Compute the required width of the data structure to achieve an approximate 1% error rate using the following relationship:     \n",
    ">    $$width = 2^{log_2(e/\\epsilon)}$$       \n",
    ">    To perform this calculation use the Python `math.log2` function and the `math.ceil` function to round up the next highest integer. Display the computed result\n",
    "> 3. Compute the required depth to ensure a less than 1% probability, $\\delta$, of breaching the error bound using the following relationship:\n",
    ">    $$depth = ln(1/\\delta)$$\n",
    ">    Use the `math.ceil` function to round up to the next highest integer. Display the computed result.    \n",
    "> 5. Instantiate a HeavyHitters object with the arguments computed above and `num_hitters = 20`.\n",
    "> 6. Iterative over the events in the stream applying the `add` method for each event, with the num_els argument set to 1. Make sure you coerce the integer identifier to a string.  \n",
    "> 7. Save the `heavy_hitters` attribute to a variable. The data structure for this attribute is a dictionary.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa6a57-bb87-40a1-a61c-33b4ec8af8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put you code below\n",
    "\n",
    "\n",
    "\n",
    "heavy_hitters = heavy_hitters.heavy_hitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf278549-14c5-4d03-bd8e-b39472a07ea4",
   "metadata": {},
   "source": [
    "> 4. Execute the provided function, using the `heavy_hitters` attribute variable you have saved.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb680e29-0953-4bf9-b444-c2250cf4cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dist(hitters):\n",
    "    heavy_hitters = np.flip(np.sort(list(hitters.values()))) \n",
    "    print('The ordered list of heavy hitters:')\n",
    "    print(heavy_hitters)  \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    ax.plot([x + 1 for x in range(len(heavy_hitters))], heavy_hitters)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_xlabel('Ordered event')\n",
    "    ax.set_title('Ordered event frequencies')\n",
    "\n",
    "display_dist(heavy_hitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ed908-56b8-400f-a953-90ffd88bdf3b",
   "metadata": {},
   "source": [
    "> Now answer the following questions.\n",
    "> 1. Does the distribution of event counts for the top 20 identifiers appear to follow the expected distribution.      \n",
    "> 2. Compute the required memory for the count-min-hash data structure, assuming 4-byte (32 bit) counters.      \n",
    "> 3. How does the memory required for the count-min-hash data structure compare to using a simple linear (or flat) table for the $10^5$ event identifiers, assuming 4-byte (32 bit) counters? Keeping in mind that $10^5$ event identifiers, what does your comparison mean for the memory efficiency of the count-min-sketch algorithm?       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162309c2-6c3d-48ad-8540-5ff7afdf2cf4",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.                 \n",
    "> 2.            \n",
    "> 3.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b9bd6-dbf8-4920-b5bd-8406a6c174ec",
   "metadata": {},
   "source": [
    "## HyperLogLog Algorithm   \n",
    "\n",
    "The HyperLogLog algorithm and its derivatives are fast and efficient methods for determining the cardinality of events in streams. The HyperLogLog uses the harmonic mean of a set of cardinality estimates. Each estimate is based on a hash-based sketch of cardinality.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d602f-70b1-442f-86a5-161dd18a49de",
   "metadata": {},
   "source": [
    "> **Exercise 02-13:** You will now work with the HyperLogLog algorithm using the Python [Datasketch package](https://ekzhu.com/datasketch/index.html). Do the following:      \n",
    "> 1. Start with the typical value of $p=14$ (confusingly precision by the creators of the DataSketch package), resulting in the number of registers, $m = 2^{14}$.     \n",
    "> 2. Compute and display the expected error rate, $\\epsilon$, given the value of $p$.      \n",
    "> 3. Instantiate a HyperLogLog object using the value of $p$ previously specified as the argument.      \n",
    "> 4. Assuming that 1-byte registers are used for accumulating the counts, compute and display the size of the HyperLogLog data structure. You can find the size of this data structure using the Python `len` function.       \n",
    "> 5. Iterate over all the events in the stream applying the `update` method to the HyperLogLog object.      \n",
    "> 6. Display the `count` attribute of the HyperLogLog object, which contains the estimated cardinality of the stream.                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e20e9e-50ac-4c5f-8227-b1accd36508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ab4e7-3e4a-4012-b486-afff573bd94b",
   "metadata": {},
   "source": [
    "> Now, answer these questions:     \n",
    "> 1. Based on the estimated error and the estimated cardinality value computed, is the error within the expected range?   \n",
    "> 2. How much memory would be required if you needed to find the cardinality of $2^{14}=16384$ streams in a large scale application?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b564757-3096-487e-8b3c-898a020506a9",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.       \n",
    "> 2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f868cca",
   "metadata": {},
   "source": [
    "#### Copyright, 2021, 2022, 2023, 2024, 2025, Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592fa46-311f-4b26-8968-d51f3989c421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
